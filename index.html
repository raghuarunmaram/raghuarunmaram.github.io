<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Raghu Arun Maram - Data Engineer Portfolio</title>
    <link rel="stylesheet" href="styles.css">
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f7fa;
            color: #333;
        }
        header {
            background: linear-gradient(135deg, #1e3c72, #2a5298);
            color: white;
            text-align: center;
            padding: 2rem 1rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        header h1 {
            margin: 0;
            font-size: 2.5rem;
        }
        header p {
            margin: 0.5rem 0;
            font-size: 1.2rem;
        }
        header a {
            color: #a1c4fd;
            text-decoration: none;
            margin: 0 10px;
            transition: color 0.3s;
        }
        header a:hover {
            color: #fff;
        }
        section {
            max-width: 900px;
            margin: 2rem auto;
            padding: 0 1rem;
        }
        h2 {
            color: #1e3c72;
            border-bottom: 2px solid #2a5298;
            padding-bottom: 0.5rem;
            font-size: 1.8rem;
        }
        #about p {
            background: white;
            padding: 1.5rem;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
        }
        #skills ul {
            list-style: none;
            padding: 0;
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 1rem;
        }
        #skills li {
            background: white;
            padding: 1rem;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
            transition: transform 0.2s;
        }
        #skills li:hover {
            transform: translateY(-5px);
        }
        #skills strong {
            color: #2a5298;
        }
        .project {
            background: white;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
            transition: box-shadow 0.3s;
        }
        .project:hover {
            box-shadow: 0 6px 12px rgba(0, 0, 0, 0.1);
        }
        .project h3 {
            color: #1e3c72;
            margin-top: 0;
            font-size: 1.5rem;
        }
        .project p {
            margin: 0.5rem 0;
        }
        .project strong {
            color: #2a5298;
        }
        footer {
            text-align: center;
            padding: 1rem;
            background: #1e3c72;
            color: white;
            position: relative;
            bottom: 0;
            width: 100%;
        }
    </style>
</head>
<body>
    <header>
        <h1>Raghu Arun Maram</h1>
        <p>Data Engineer | 5+ Years of Experience | AWS & Big Data Specialist</p>
        <p>
            <a href="https://linkedin.com/in/raghuarunmaram">LinkedIn</a> | 
            <a href="mailto:raghuarun0912@gmail.com">raghuarun0912@gmail.com</a>
        </p>
    </header>

    <section id="about">
        <h2>About Me</h2>
        <p>I’m a seasoned Data Engineer with over 5 years of experience architecting robust, scalable data ecosystems. At Amazon Web Services (AWS), I spent 3 years crafting high-throughput pipelines using Kinesis, Glue, and Redshift, reducing latency by up to 40% for real-time analytics. Previously, at TCS, I optimized ETL workflows across 2 years, migrating terabyte-scale datasets to cloud platforms like Snowflake. I thrive on transforming complex raw data into actionable insights with cutting-edge cloud and big data technologies.</p>
    </section>

    <section id="skills">
        <h2>Key Skills</h2>
        <ul>
            <li><strong>AWS Ecosystem</strong>: S3 (Partitioned Parquet Storage), Redshift (Advanced SQL), Glue (Crawlers, DynamicFrames, Jobs), EMR (PySpark Clusters), SageMaker (Model Integration), CloudWatch (Monitoring), Lambda (Shard Automation), Kinesis (Streams, Firehose), DataSync (HDFS/NFS Ingestion), Athena (Query Optimization), CloudFormation (IaC), SDK (Boto3)</li>
            <li><strong>Programming & Scripting</strong>: Python (Pandas, Boto3, Flask), SQL (JOINs, Window Functions, Tuning), PySpark (Distributed Processing), YAML (Infrastructure Templating)</li>
            <li><strong>Big Data Frameworks</strong>: Apache Spark (ETL Optimization), Databricks (Delta Lake, Notebooks), HDFS (Legacy Integration)</li>
            <li><strong>ETL & Pipeline Engineering</strong>: AWS Glue (Event-Driven ETL), Apache Airflow (Workflow Orchestration), Fivetran (Data Ingestion), Matillion (Snowflake ETL), Tableau Prep (Data Shaping), Real-Time & Batch Processing</li>
            <li><strong>Data Warehousing</strong>: Snowflake (Data Exchange, SQL Functions), Redshift (Scalable Analytics), SQL Server (Staging Areas), Schema Evolution & Modeling</li>
            <li><strong>Data Processing</strong>: DynamicFrames (Glue Transformations), Parquet Management, Data Validation, Query & Pipeline Optimization (30-50% Gains)</li>
            <li><strong>Analytics & BI Tools</strong>: Tableau (Dashboards, Calculated Fields), Power BI (Reporting), Athena (Ad-Hoc Insights)</li>
            <li><strong>Automation</strong>: Lambda (Dynamic Scaling), Glue Triggers (Event Automation), Azure Data Factory (Job Scheduling), Airflow (Pipeline Management)</li>
            <li><strong>Security</strong>: Encryption (Data-at-Rest), Access Controls (IAM), Compliance (Privacy Policies)</li>
            <li><strong>Database Expertise</strong>: Stored Procedures (SQL), Indexing (Performance), Query Tuning (Explain Plans)</li>
        </ul>
    </section>

    <section id="projects">
        <h2>Projects</h2>
        <div class="project">
            <h3>Real-Time Sales Analytics Pipeline (Amazon Web Services)</h3>
            <p>Engineered a high-performance ETL pipeline to process 10TB/day of Amazon Ads transactional data, integrating real-time ingestion via AWS Kinesis Data Streams (handling 5M events/second) with AWS Glue for automated schema inference using Crawlers. Wrote Python scripts leveraging Boto3 and Pandas for multi-stage transformations, partitioning data into Parquet files stored in S3 for scalability. Loaded processed data into Redshift with optimized SQL queries (nested JOINs, window functions), slashing query latency by 40% and enabling 50+ business analysts to access near-real-time dashboards.</p>
            <p><strong>Tech:</strong> AWS Glue (Crawlers, Jobs), Amazon Redshift, Python (Pandas, Boto3), Amazon S3 (Parquet), SQL</p>
        </div>
        <div class="project">
            <h3>Data Migration for Legacy Systems (TCS)</h3>
            <p>Orchestrated the zero-downtime migration of 5TB of heterogeneous data from on-premises SQL Server databases to Snowflake, utilizing Apache Airflow for workflow automation and Python scripts for extracting data from REST APIs and flat files. Staged data in S3 buckets, transformed it with Snowflake’s native SQL functions (e.g., LATERAL FLATTEN for JSON), and optimized target schemas with indexing, achieving a 30% boost in query performance. Integrated the warehouse with Tableau, delivering interactive KPI dashboards to stakeholders.</p>
            <p><strong>Tech:</strong> SQL (SQL Server, Snowflake), Python, Apache Airflow, AWS S3, Snowflake, Tableau</p>
        </div>
    </section>

    <footer>
        <p>© 2025 Raghu Arun Maram. Hosted on GitHub Pages.</p>
    </footer>
</body>
</html>